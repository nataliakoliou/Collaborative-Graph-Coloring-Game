@book{
Shoham_Leyton-Brown_2008,
place={Cambridge}, 
title={Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations},
publisher={Cambridge University Press}, 
author={Shoham, Yoav and Leyton-Brown, Kevin}, 
year={2008}
}

@article{
doi:10.1073/pnas.36.1.48,
author = {John F. Nash },
title = {Equilibrium points in <i>n</i>-person games},
journal = {Proceedings of the National Academy of Sciences},
volume = {36},
number = {1},
pages = {48-49},
year = {1950},
doi = {10.1073/pnas.36.1.48},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.36.1.48},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.36.1.48}
}

@misc{
omidshafiei2019alpharank,
title={$\alpha$-Rank: Multi-Agent Evaluation by Evolution}, 
author={Shayegan Omidshafiei and Christos Papadimitriou and Georgios Piliouras and Karl Tuyls and Mark Rowland and Jean-Baptiste Lespiau and Wojciech M. Czarnecki and Marc Lanctot and Julien Perolat and Remi Munos},
year={2019},
eprint={1903.01373},
archivePrefix={arXiv},
primaryClass={cs.MA}
}

@article{
dynamicgames/krawczyk-jacek,
author = {Krawczyk, Jacek},
year = {1998},
month = {01},
pages = {},
title = {An Introduction to Dynamic Games}
}

@article{
Shapley1953StochasticG,
title={Stochastic Games*},
author={Lloyd S. Shapley},
journal={Proceedings of the National Academy of Sciences},
year={1953},
volume={39},
pages={1095 - 1100},
url={https://api.semanticscholar.org/CorpusID:263414073}
}

@inproceedings{
Levet2016GameT,
title={Game Theory : Normal Form Games},
author={Michael Levet},
year={2016},
url={https://api.semanticscholar.org/CorpusID:131771375}
}

@misc{
wellman2024empiricalgametheoreticanalysissurvey,
title={Empirical Game-Theoretic Analysis: A Survey}, 
author={Michael P. Wellman and Karl Tuyls and Amy Greenwald},
year={2024},
eprint={2403.04018},
archivePrefix={arXiv},
primaryClass={cs.GT},
url={https://arxiv.org/abs/2403.04018}, 
}

@misc{
paul2022multiagentpathfinding,
title={Multi Agent Path Finding using Evolutionary Game Theory}, 
author={Sheryl Paul and Jyotirmoy V. Deshmukh},
year={2022},
eprint={2212.02010},
archivePrefix={arXiv},
primaryClass={cs.MA},
url={https://arxiv.org/abs/2212.02010},
}

@article{
David_2014,
title={Genetic Algorithms for Evolving Computer Chess Programs},
volume={18},
ISSN={1941-0026},
url={http://dx.doi.org/10.1109/TEVC.2013.2285111},
DOI={10.1109/tevc.2013.2285111},
number={5},
journal={IEEE Transactions on Evolutionary Computation},
publisher={Institute of Electrical and Electronics Engineers (IEEE)},
author={David, Omid E. and van den Herik, H. Jaap and Koppel, Moshe and Netanyahu, Nathan S.},
year={2014},
month=oct, pages={779–789}
}

@misc{
watkins2023generating,
title={Generating a Graph Colouring Heuristic with Deep Q-Learning and Graph Neural Networks}, 
author={George Watkins and Giovanni Montana and Juergen Branke},
year={2023},
eprint={2304.04051},
archivePrefix={arXiv},
primaryClass={cs.LG}
}

@article{
10.5555/2831071.2831085,
author = {Bloembergen, Daan and Tuyls, Karl and Hennes, Daniel and Kaisers, Michael},
title = {Evolutionary dynamics of multi-agent learning: a survey},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {The interaction of multiple autonomous agents gives rise to highly dynamic and nondeterministic environments, contributing to the complexity in applications such as automated financial markets, smart grids, or robotics. Due to the sheer number of situations that may arise, it is not possible to foresee and program the optimal behaviour for all agents beforehand. Consequently, it becomes essential for the success of the system that the agents can learn their optimal behaviour and adapt to new situations or circumstances. The past two decades have seen the emergence of reinforcement learning, both in single and multi-agent settings, as a strong, robust and adaptive learning paradigm. Progress has been substantial, and a wide range of algorithms are now available. An important challenge in the domain of multi-agent learning is to gain qualitative insights into the resulting system dynamics. In the past decade, tools and methods from evolutionary game theory have been successfully employed to study multi-agent learning dynamics formally in strategic interactions. This article surveys the dynamical models that have been derived for various multi-agent reinforcement learning algorithms, making it possible to study and compare them qualitatively. Furthermore, new learning algorithms that have been introduced using these evolutionary game theoretic tools are reviewed. The evolutionary models can be used to study complex strategic interactions. Examples of such analysis are given for the domains of automated trading in stock markets and collision avoidance in multi-robot systems. The paper provides a roadmap on the progress that has been achieved in analysing the evolutionary dynamics of multi-agent learning by highlighting the main results and accomplishments.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {659–697},
numpages = {39}
}

@inproceedings{
Sinha_2023,
title={A Linear Programming Enhanced Genetic Algorithm for Hyperparameter Tuning in Machine Learning},
url={http://dx.doi.org/10.1109/CEC53210.2023.10254162},
DOI={10.1109/cec53210.2023.10254162},
booktitle={2023 IEEE Congress on Evolutionary Computation (CEC)},
publisher={IEEE},
author={Sinha, Ankur and Pankaj, Paritosh},
year={2023},
month=jul, pages={1–8} 
}

@misc{
ganapathy2020studygeneticalgorithmshyperparameter,
title={A Study of Genetic Algorithms for Hyperparameter Optimization of Neural Networks in Machine Translation}, 
author={Keshav Ganapathy},
year={2020},
eprint={2009.08928},
archivePrefix={arXiv},
primaryClass={cs.NE},
url={https://arxiv.org/abs/2009.08928}, 
}

@article{
10.1007/BF00992699,
author = {Lin, Long-Ji},
title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992699},
doi = {10.1007/BF00992699},
journal = {Mach. Learn.},
month = {may},
pages = {293–321},
numpages = {29},
keywords = {Reinforcement learning, connectionist networks, planning, teaching}
}

@misc{
vanhasselt2015deep,
title={Deep Reinforcement Learning with Double Q-learning}, 
author={Hado van Hasselt and Arthur Guez and David Silver},
year={2015},
eprint={1509.06461},
archivePrefix={arXiv},
primaryClass={cs.LG}
}