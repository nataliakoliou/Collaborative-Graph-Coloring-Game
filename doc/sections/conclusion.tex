\subsection{Conclusion}

    In this study, we developed a methodology for identifying strong joint-strategies in dynamic multi-agent games, accounting for stability and performance, using the \emph{$\alpha$-Rank} evolutionary algorithm. The methodology is applied on a stochastic version of the \emph{Graph Coloring Problem}, in which players work together to color a graph while ensuring that neighboring vertices are assigned different colors. According to the methodology, first we transformed the game into its empirical form, by defining strategies (styles of play). We then designed and trained Deep Q-Learning policy models that realize those styles of play in the underlying game, and run simulations to generate the empirical payoff matrix. \emph{$\alpha$-Rank}, applied to this matrix, results into a unique stationary distribution over strategy profiles that defines the empirical game's MCC. The \emph{$\alpha$-Rank} not only helped us identify stable strategy profiles resistant to changes but also provided a descriptive framework for understanding why certain profiles prevail in the long run, based on the underlying dynamics of the game. Through this approach, we successfully described a concise methodology for evaluating and ranking agents' joint policies, considering their long-term interactions in dynamic settings, while also explaining how strategy profiles are defined within the MCC.\tinydouble

    \noindent
    Future work involves (a) applying the methodology in more complex and large-scale settings, accounting for strategy profiles of multiple stakeholders that may collaborate and/or compete, (b) using machine learning methods, such as imitation learning and inverse reinforcement learning, to identify different styles of play from demonstrations and specifying the empirical game, (c) exploring advanced models able to adapt their strategies based on observed behaviors based on the behavior of co-players, (d) testing the methodology across various configurations (e.g., different graph structures and sparsities) to evaluate robustness, and (e) applying the methodology into real-world settings where agents need to align with human preferences in dynamic settings.